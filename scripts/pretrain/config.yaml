# Logging configuration
experiment_name: acegen
agent_name: pretrain_gru
logger_backend: null # csv, wandb, tensorboard, or null
seed: 101
dataset_log_dir: /tmp/pretrain
model_log_dir: /tmp/pretrain

# Dataset configuration
train_dataset_path: /shared/morgan/mol_opt/data/zinc.txt
tokenizer: SMILESTokenizer # SMILESTokenizer, SMILESTokenizer2, DeepSMILESTokenizer, SELFIESTokenizer, AISTokenizer, SAFETokenizer, SmiZipTokenizer

# Model configuration
model: gpt2 # gru, lstm, or gpt2

# Training configuration
lr: 0.0001
lr_decay_per_epoch: 1.0 # no decay
epochs: 10
batch_size: 8
randomize_smiles: False
num_test_smiles: 100
